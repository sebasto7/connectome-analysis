{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fbbfb0d",
   "metadata": {},
   "source": [
    "# Updating a proofreading table or a list id file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5355cfd2",
   "metadata": {},
   "source": [
    "This notebook contains functions and example scripts to update a proofreading table or lists of ids stored in an excel or txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ccbf1d",
   "metadata": {},
   "source": [
    "Content per section:\n",
    "<br>A) Updating IDs and presynaptic counts\n",
    "<br>B) Updating main postsynaptic neurons database (table)\n",
    "<br>B2) Updating all postsynaptic neurons databases (loop)\n",
    "<br>C) Updating a list of IDs from a txt file\n",
    "<br>D) Updating a list of IDs from any excel file\n",
    "<br>E) Making a list of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e090bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import fafbseg\n",
    "import math\n",
    "import itertools\n",
    "from fafbseg import flywire\n",
    "from caveclient import CAVEclient\n",
    "from helpers.synapse_queries import combine_xyz, separate_xyz, synapse_count, filter_points, calculate_distance\n",
    "client = CAVEclient('flywire_fafb_production')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed0ec97",
   "metadata": {},
   "source": [
    "# A) Updating IDs and presynaptic counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f4beeb",
   "metadata": {},
   "source": [
    "## 1. Loading X neuron input neuron table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15183aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose path and file\n",
    "dataPath = r'Z:\\Further projects\\Heterogeneity across cell types\\data\\Excels\\drive-data-sets' # your path \n",
    "fileName = f'Mi1_neurons_input_count_R_20240610.xlsx' # the file you want to update\n",
    "filePath = os.path.join(dataPath,fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079d3f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading file as DataFrame\n",
    "df = pd.read_excel(filePath)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9179bf72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dropping the fisrt row ('asdf' was added as a walk-around to set that column values as type str)\n",
    "if df[\"postsynaptic_ID\"][0] == 'asdf': \n",
    "    df = df.iloc[1: , :]\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "#df = df[df['counts'] >= 3].copy() # Removing rows based on absolute count filter\n",
    "    \n",
    "#Getting the lists of IDs to update\n",
    "segmentIDs = df[\"seg_id\"].copy()\n",
    "pre_IDs = df[\"presynaptic_ID\"].copy()\n",
    "post_IDs = df[\"postsynaptic_ID\"].copy()\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d06e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the 'INPUTS PROOFREAD' labelled row of the lists for the previous postsynaptic cell ID\n",
    "# An old format of the files in the Tm9 project had such \"INPUTS PROOFREAD\" lines. They needed to be discarded\n",
    "\n",
    "for i, id in enumerate(pre_IDs):\n",
    "    if id == 'INPUTS PROOFREAD':\n",
    "        segmentIDs[i] = post_IDs[i-1] # The previous postsynaptic cell ID\n",
    "        pre_IDs[i] = post_IDs[i-1]\n",
    "        post_IDs[i] = post_IDs[i-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e66906b",
   "metadata": {},
   "source": [
    "## 2. Updating IDs considering our excel file extructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a205648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating segments (trying to update in loops to not collapse the server)\n",
    "\n",
    "confidence_of_update_pre = []\n",
    "confidence_of_update_post = []\n",
    "updated_presynaptic_ID_column = []\n",
    "updated_postsynaptic_ID_column = []\n",
    "pre_ID_i = 0\n",
    "post_ID_i = 0\n",
    "rounds_of = 100\n",
    "curr_round = 0\n",
    "print(f'Total rounds to perform: {math.ceil((len(pre_IDs)/rounds_of))}')\n",
    "\n",
    "for i in range(0,math.ceil((len(pre_IDs)/rounds_of))):\n",
    "    curr_round += 1\n",
    "    \n",
    "    #pre_IDs\n",
    "    curr_pre_IDs = pre_IDs[pre_ID_i:pre_ID_i+rounds_of]\n",
    "    temp_segmentIDs_df = flywire.update_ids(curr_pre_IDs.tolist(), stop_layer=2, supervoxels=None, timestamp=None, dataset='production', progress=True)\n",
    "    updated_presynaptic_ID_column.append(temp_segmentIDs_df[\"new_id\"].tolist())\n",
    "    confidence_of_update_pre.append(temp_segmentIDs_df[\"confidence\"].tolist())\n",
    "    pre_ID_i +=rounds_of\n",
    "    \n",
    "    #post_IDs\n",
    "    curr_post_IDs = post_IDs[post_ID_i:post_ID_i+rounds_of]\n",
    "    temp_segmentIDs_df = flywire.update_ids(curr_post_IDs.tolist(), stop_layer=2, supervoxels=None, timestamp=None, dataset='production', progress=True)\n",
    "    updated_postsynaptic_ID_column.append(temp_segmentIDs_df[\"new_id\"].tolist())\n",
    "    confidence_of_update_post.append(temp_segmentIDs_df[\"confidence\"].tolist())\n",
    "    post_ID_i +=rounds_of\n",
    "    \n",
    "    if curr_round%5 == 0: #printing current round every 5 rounds\n",
    "        print(curr_round)\n",
    "\n",
    "updated_presynaptic_ID_column = list(itertools.chain.from_iterable(updated_presynaptic_ID_column))\n",
    "updated_postsynaptic_ID_column = list(itertools.chain.from_iterable(updated_postsynaptic_ID_column))\n",
    "confidence_of_update_pre = list(itertools.chain.from_iterable(confidence_of_update_pre))\n",
    "confidence_of_update_post = list(itertools.chain.from_iterable(confidence_of_update_post))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ab5d14",
   "metadata": {},
   "source": [
    "### 2.1 Adding important columns for next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bef476",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting important columns to str\n",
    "df[\"presynaptic_ID\"] = df[\"presynaptic_ID\"].astype(str)\n",
    "df[\"postsynaptic_ID\"] = df[\"postsynaptic_ID\"].astype(str)\n",
    "df[\"seg_id\"] = df[\"seg_id\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Updating the dataframe\n",
    "# Adding the new url column to the data frame\n",
    "df[\"Updated_presynaptic_ID\"] = updated_presynaptic_ID_column\n",
    "df[\"Updated_presynaptic_ID\"] = df[\"Updated_presynaptic_ID\"].astype(str) \n",
    "df[\"Updated_postsynaptic_ID\"] = updated_postsynaptic_ID_column\n",
    "df[\"Updated_postsynaptic_ID\"] = df[\"Updated_postsynaptic_ID\"].astype(str) \n",
    "df[\"Update_confidence_pre\"] = confidence_of_update_pre\n",
    "df[\"Update_confidence_post\"] = confidence_of_update_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8341a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(confidence_of_update_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297c7d69",
   "metadata": {},
   "source": [
    "## 3. Updating counts between pre- and post synaptic partners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c64456",
   "metadata": {},
   "source": [
    "Strategy to save time:\n",
    "1. (Step1) Create a dictionary with postsynaptic neuron's IDs as KEYS and their a input-neuron dataframe as VALUES , and then (step2) create a loop across presynaptic IDs to get the exact counting from the input-neuron-dataframe of postsynaptic neurons, loading the correct input-neuron-dataframe from the dictionary each time.\n",
    "\n",
    "    Or, all in one single step: start a loop across unique postsynaptic IDs (be careful that the order is in the same as in the excel file, print them), get the input-neuron dataframe per each one in each round of the loop, and add a second loop across presynaptic IDs to get the exact counting.\n",
    "    \n",
    "\n",
    "2. Save the new countings in order of production in the same dataframe, as well as a column showing duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e63cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing the \"single step\" option\n",
    "\n",
    "unique_post_IDs = df[df['presynaptic_ID'] != 'INPUTS PROOFREAD']['postsynaptic_ID'].unique()\n",
    "new_count_ls =  [] # For storing the new counts\n",
    "copy_count_ls = [] # For storing the copy counts \n",
    "for post_id in unique_post_IDs: # loop across postsynaptic ids.\n",
    "    if flywire.is_latest_root([post_id])[0]:\n",
    "        curr_id = post_id\n",
    "        print(f'Getting inputs from: {curr_id}')\n",
    "    else:\n",
    "        updated_ID_df = flywire.update_ids(post_id, stop_layer=2, supervoxels=None, timestamp=None, dataset='production', progress=True)\n",
    "        curr_id = updated_ID_df[\"new_id\"][0]\n",
    "        print(f'Getting inputs from: {curr_id} (updated)')\n",
    "    \n",
    "    curr_df = df[df['postsynaptic_ID'] == post_id].copy()\n",
    "    curr_neurons_inputs = flywire.synapses.fetch_synapses(curr_id, pre=False, post=True, attach=True, \n",
    "                                             min_score=50, clean=True, transmitters=False, \n",
    "                                             neuropils=True, batch_size=30, \n",
    "                                             dataset='production', progress=True,mat= \"live\")\n",
    "    \n",
    "    if curr_neurons_inputs.empty: #Adding this to fix isues with retrieveing data\n",
    "        #Adding NaNs\n",
    "        empty_list = [None]* len(df[df['postsynaptic_ID'] == post_id])\n",
    "        new_count_ls = new_count_ls + empty_list\n",
    "        copy_count_ls = copy_count_ls + empty_list\n",
    "    \n",
    "    else: \n",
    "        \n",
    "        #Filtering redundant / duplicated counts (threshold = 100 nm)\n",
    "        combine_xyz(curr_neurons_inputs)\n",
    "        points = curr_neurons_inputs['pre_pt_position'].tolist()\n",
    "        points_no_duplicates = filter_points(points, threshold_distance = 100)\n",
    "        curr_neurons_inputs_no_duplicates = curr_neurons_inputs[curr_neurons_inputs['pre_pt_position'].apply(lambda x: x in points_no_duplicates)].copy()\n",
    "\n",
    "        pre_id_copies_dict = {} # For checking ID duplicates, triplicates, etc\n",
    "        for i,pre_id in enumerate(curr_df[\"Updated_presynaptic_ID\"]): # loop across presynaptic ids\n",
    "            #Counting copies\n",
    "            if pre_id in pre_id_copies_dict.keys():\n",
    "                pre_id_copies_dict[pre_id]+= 1 # duplicates, triplicates ...\n",
    "            else:\n",
    "                pre_id_copies_dict[pre_id] = 1 # initial count\n",
    "\n",
    "            c = len(curr_neurons_inputs_no_duplicates[curr_neurons_inputs_no_duplicates['pre_pt_root_id'] == int(pre_id)])\n",
    "            new_count_ls.append(c) # count between specific pre and post\n",
    "            copy_count_ls.append(pre_id_copies_dict[pre_id])\n",
    "            #print(f'Counts with {pre_id}: {c}, confidence {curr_df[\"Update_confidence\"].tolist()[i]}, copy {pre_id_copies_dict[pre_id]}')\n",
    "\n",
    "        #In old files with this rows acting as separators\n",
    "        #new_count_ls.append('INPUTS PROOFREAD')\n",
    "        #copy_count_ls.append('INPUTS PROOFREAD')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b769853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new columns to the main dataframe\n",
    "df[\"Updated_counts\"] = new_count_ls\n",
    "df[\"duplicates\"] =  \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1db6679",
   "metadata": {},
   "source": [
    "## 4. Saving back to the excell file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895a22bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating string for the date\n",
    "\n",
    "import datetime\n",
    "x = datetime.datetime.now()\n",
    "date_str = x.strftime(\"%d\") + x.strftime(\"%b\") + x.strftime(\"%Y\")\n",
    "\n",
    "# Writting in an existing excel file\n",
    "from openpyxl import load_workbook\n",
    "book = load_workbook(filePath)\n",
    "writer = pd.ExcelWriter(filePath, engine = 'openpyxl')\n",
    "writer.book = book\n",
    "\n",
    "df.to_excel(writer, sheet_name='Updated_dataframe_'+date_str)\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b26163",
   "metadata": {},
   "source": [
    "### 3. Or, saving in a new excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55348263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving in a new file\n",
    "# Specify the corect file_name\n",
    "\n",
    "import datetime\n",
    "x = datetime.datetime.now()\n",
    "date_str = x.strftime(\"%d\") + x.strftime(\"%b\") + x.strftime(\"%Y\")\n",
    "\n",
    "file_name = f'Mi1_neurons_input_count_segments_update_{date_str}.xlsx'\n",
    "savePath = os.path.join(dataPath, file_name)\n",
    "df.to_excel(savePath, sheet_name='Segments update')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "savePath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33a2028",
   "metadata": {},
   "source": [
    "# B) Updating main postsynaptic neurons database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d77f535",
   "metadata": {},
   "source": [
    "## 1. Loading postsynaptic neuron dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose path and file\n",
    "PC_disc = 'D'\n",
    "dataPath = f'{PC_disc}:\\Connectomics-Data\\FlyWire\\Excels\\drive-data-sets\\database'\n",
    "fileName = f'Tm9 proofreadings.xlsx'\n",
    "filePath = os.path.join(dataPath,fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1b0994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading file as DataFrame\n",
    "df = pd.read_excel(filePath)\n",
    "if df[\"seg_id\"][0] == 'asdf': #Dropping the fisrt row ('asdf' was added as a walk-around to set that column values as type str)\n",
    "    df = df.iloc[1: , :]\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "df = df.astype(str)\n",
    "display(df.head())\n",
    "segmentIDs = df[\"Updated_seg_id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86065a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(segmentIDs[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df911c6d",
   "metadata": {},
   "source": [
    "## 2. Update with FAFB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d797c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the segmentIDs\n",
    "new_segmentIDs_df = flywire.update_ids(segmentIDs, stop_layer=2, supervoxels=None, timestamp=None, dataset='production', progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ee322",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_segmentIDs_df[\"confidence\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Updating the dataframe\n",
    "# Adding the new column to the data frame\n",
    "df[\"Update_confidence\"] = new_segmentIDs_df[\"confidence\"].astype(str).tolist()\n",
    "df[\"Updated_seg_id\"] = new_segmentIDs_df[\"new_id\"].astype(str).tolist()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6b295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking duplicates in ids\n",
    "df[df[\"Updated_seg_id\"].duplicated() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1029b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking updated ids\n",
    "df[df[\"Update_confidence\"].astype(float) < 1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271c1dff",
   "metadata": {},
   "source": [
    "## 3. If wished: Reorder rows based on condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c5223",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values(by = 'cluster_id').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37a7fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140c3852",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"seg_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187aae5",
   "metadata": {},
   "source": [
    "## 3. If wished: Add specific column status based on another file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f604b",
   "metadata": {},
   "source": [
    "### 3.1 Based on a txt with list of ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2272e4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting column for the update and file from which the info comes\n",
    "column_to_update = 'rim_area (Y/N)' # 'inputs_proofread (Y/N)', detached_lamina (Y/N), 'healthy_L3 (Y/N)', 'rim_area (Y/N)'\n",
    "\n",
    "update_file_path = r'D:\\Connectomics-Data\\FlyWire\\Txts\\cell_type_proofread'\n",
    "update_file_with = 'root_ids_Tm9_outer_rim_20231018.txt' # list of ids\n",
    "\n",
    "update_filePath =os.path.join(update_file_path,update_file_with)\n",
    "\n",
    "# extractring info from the specific file\n",
    "update_file_with_df = pd.read_csv(update_filePath)\n",
    "update_file_with_ids_list = update_file_with_df.columns.tolist()\n",
    "\n",
    "# Updating the list \n",
    "df[f'{column_to_update}_updated'] = np.where(df['seg_id'].isin(update_file_with_ids_list), \"Y\", \"N\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf9e85",
   "metadata": {},
   "source": [
    "### 3.2 Or, Based on excel files with ids in a column and extra information in other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21744074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting column for the update and file from which the info comes\n",
    "#For XYZ columns\n",
    "update_file_path = r'E:\\Connectomics-Data\\FlyWire\\Excels'\n",
    "update_file_with = f'XYZ_df_{date}.xlsx'\n",
    "update_filePath =os.path.join(update_file_path,update_file_with)\n",
    "\n",
    "# extractring info from the specific file\n",
    "update_file_with_df = pd.read_excel(update_filePath)\n",
    "update_file_with_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45b9b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transfering info from one dataframe to another based on a reference column (here seg_id)\n",
    "def update_dataframe_single_column(source_df, target_df, reference_column):\n",
    "    # Create a dictionary mapping from the reference column to the source DataFrame\n",
    "    reference_dict = source_df.groupby(reference_column).first().reset_index().to_dict(orient='records')\n",
    "    reference_dict = {row[reference_column]: row for row in reference_dict}\n",
    "\n",
    "    # Update the target DataFrame based on the reference column\n",
    "    for i, row in target_df.iterrows():\n",
    "        ref = row[reference_column]\n",
    "        if ref in reference_dict:\n",
    "            source_row = reference_dict[ref]\n",
    "            target_df.loc[i] = source_row\n",
    "\n",
    "    return target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f03a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function inputs\n",
    "source_cols = ['XYZ-ME', 'XYZ-LO','seg_id']\n",
    "target_cols = ['XYZ-ME', 'XYZ-LO','seg_id']\n",
    "reference_column = 'seg_id'\n",
    "\n",
    "# Selecting dataframes and resetting index\n",
    "source_df = update_file_with_df[source_cols].copy()\n",
    "source_df.reset_index(inplace = True, drop = True)\n",
    "target_df = df[target_cols].copy()\n",
    "target_df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "\n",
    "source_df = source_df.astype(str)\n",
    "target_df = target_df.astype(str)\n",
    "\n",
    "# Running the function and compleating the dataset\n",
    "result_df = update_dataframe_single_column(source_df, target_df,reference_column)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b5f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating string for the date\n",
    "import datetime\n",
    "x = datetime.datetime.now()\n",
    "date_str = x.strftime(\"%d\") + x.strftime(\"%b\") + x.strftime(\"%Y\")\n",
    "\n",
    "# Writting in an existing excel file\n",
    "from openpyxl import load_workbook\n",
    "book = load_workbook(filePath)\n",
    "writer = pd.ExcelWriter(filePath, engine = 'openpyxl')\n",
    "writer.book = book\n",
    "\n",
    "result_df = result_df.astype(str)\n",
    "result_df.to_excel(writer, sheet_name='Updated_table_'+date_str) #sorted_df\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739fef16",
   "metadata": {},
   "source": [
    "## 3. If wished: Add the center of mass of postsynaptic sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0981b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function\n",
    "def find_center_point(points, threshold):\n",
    "    if isinstance(points, list):\n",
    "        points = np.array(points)\n",
    "\n",
    "    # Calculate the distances between each point and all other points\n",
    "    distances = np.linalg.norm(points[:, np.newaxis] - points, axis=2)\n",
    "\n",
    "    # Calculate the average distance for each point\n",
    "    avg_distances = np.mean(distances, axis=1)\n",
    "\n",
    "    # Find the indices of points within the threshold distance\n",
    "    valid_indices = np.where(avg_distances < threshold)[0]\n",
    "\n",
    "    # Check if there are any valid points\n",
    "    if len(valid_indices) > 0:\n",
    "        # Calculate the geometric center of valid points\n",
    "        center_point = np.mean(points[valid_indices], axis=0)\n",
    "        #Rounding\n",
    "        center_point = np.round(center_point, decimals=1)\n",
    "        \n",
    "        # Find the closest point to the center\n",
    "        closest_point_index = np.argmin(np.linalg.norm(points[valid_indices] - center_point, axis=1))\n",
    "        closest_point = points[valid_indices][closest_point_index]\n",
    "    else:\n",
    "        center_point = np.array([0,0,0])\n",
    "        closest_point = np.array([0,0,0])\n",
    "\n",
    "    return center_point.tolist(), closest_point.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae449638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_xyz(df):\n",
    "    \"\"\"\n",
    "    Combines separated x, y and z column into one, changes units and adds new column names for\n",
    "    generating a neuroglancer link with function nglui.statebuilder.helpers.make_synapse_neuroglancer_link\n",
    "\n",
    "    Args:\n",
    "        pandas data frame containing x,y and z as columns of the same length\n",
    "\n",
    "    Returns:\n",
    "        same pandas data frame containing a new column with [x/4,y/4,z/40] lists\n",
    "    \"\"\"\n",
    "    # Generating the single column\n",
    "\n",
    "    post_pt_position = []\n",
    "    for x,y,z in zip(df['post_x'].tolist(),df['post_y'].tolist(),df['post_z'].tolist()):\n",
    "        temp_ls = [x/4,y/4,z/40]\n",
    "        post_pt_position.append(temp_ls)\n",
    "\n",
    "    pre_pt_position = []\n",
    "    for x,y,z in zip(df['pre_x'].tolist(),df['pre_y'].tolist(),df['pre_z'].tolist()):\n",
    "        temp_ls = [x/4,y/4,z/40]\n",
    "        pre_pt_position.append(temp_ls)\n",
    "\n",
    "    #Adding new columns and names\n",
    "    df['post_pt_position'] = post_pt_position\n",
    "    df['pre_pt_position'] = pre_pt_position\n",
    "    #Changing column names\n",
    "    df.rename(columns={'pre': 'pre_pt_root_id', 'post': 'post_pt_root_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9273ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading postsynaptic coordinated for each neuron in a specific neuropile and getting the center point\n",
    "#The loop can take quite a lot of time depending on the ammount of ids!\n",
    "\n",
    "\n",
    "#Looping for several rounds of X (e.g, 100)\n",
    "rounds_of = 100\n",
    "loop_number = 1 # Must start at \"1\" if not run before\n",
    "start_point = (rounds_of*loop_number) - rounds_of\n",
    " \n",
    "#Shortening the df to priorize id:\n",
    "\n",
    "#short_df = df[(df['detached_lamina (Y/N)'] == 'N') &(df['inputs_proofread (Y/N)'] == 'Y')].copy()\n",
    "short_df = df[df['XYZ-ME'] == 'nan'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e6bde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(short_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95279bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(len(short_df)/rounds_of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43baa13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping across chosen rows\n",
    "\n",
    "import datetime\n",
    "x = datetime.datetime.now()\n",
    "date_str = x.strftime(\"%d\") + x.strftime(\"%b\") + x.strftime(\"%Y\")\n",
    "\n",
    "for i in range(loop_number, round(len(short_df)/rounds_of)+2):\n",
    "    print(f'Loop #: {loop_number}')\n",
    "    curr_df = short_df[start_point:start_point+rounds_of].copy()\n",
    "    curr_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    XYZ_ME = []\n",
    "    XYZ_LO = []\n",
    "    for seg_id in curr_df[\"Updated_seg_id\"]:\n",
    "\n",
    "        #Print\n",
    "        print(f'currently at: {seg_id}')\n",
    "        # Getting coordinates of synaptic contacts\n",
    "        neurons_inputs = flywire.synapses.fetch_synapses(seg_id, pre=False, post=True, attach=True, \n",
    "                                                     min_score=50, clean=True, transmitters=False, \n",
    "                                                     neuropils=True, batch_size=30, \n",
    "                                                     dataset='production', progress=True,mat= \"live\")\n",
    "        combine_xyz(neurons_inputs)\n",
    "        threshold = 5000\n",
    "\n",
    "        try:\n",
    "            #Find the center point with medulla coordinates\n",
    "            neurons_inputs_ME = neurons_inputs[neurons_inputs['neuropil'].str.contains('ME')].copy()\n",
    "            points = neurons_inputs_ME['post_pt_position'].tolist()\n",
    "            center_point, closest_point = find_center_point(points, threshold)\n",
    "            XYZ_ME.append(closest_point)\n",
    "        except: # to catch some point clouds that have no ME \n",
    "            XYZ_ME.append([0,0,0])\n",
    "            \n",
    "        try:\n",
    "            #Find the center point with lobula coordinates\n",
    "            neurons_inputs_LO = neurons_inputs[neurons_inputs['neuropil'].str.contains('LO')].copy()\n",
    "            points = neurons_inputs_LO['post_pt_position'].tolist()\n",
    "            center_point, closest_point = find_center_point(points, threshold)\n",
    "            XYZ_LO.append(closest_point)\n",
    "        except: # to catch some point clouds that have no LO labels\n",
    "            XYZ_LO.append([0,0,0])\n",
    "            \n",
    "\n",
    "    XYZ_LO_strings = [','.join(map(str, sublist)) for sublist in XYZ_LO]\n",
    "    XYZ_ME_strings = [','.join(map(str, sublist)) for sublist in XYZ_ME]\n",
    "\n",
    "    #saving\n",
    "    XYZ_df = pd.DataFrame(XYZ_ME_strings, columns=['XYZ-ME'])\n",
    "    XYZ_df['XYZ-LO'] = XYZ_LO_strings\n",
    "    XYZ_df['Updated_seg_id'] =  curr_df['Updated_seg_id']\n",
    "    XYZ_df['seg_id'] =  curr_df['seg_id']\n",
    "    XYZ_df.to_excel(f'D:\\Connectomics-Data\\FlyWire\\Excels\\drive-data-sets\\XYZ_df_{loop_number}_{date_str}.xlsx', index=False)\n",
    "    start_point += rounds_of\n",
    "    loop_number += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2491c222",
   "metadata": {},
   "source": [
    "### 4. Saving back to excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c3d919",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating string for the date\n",
    "import datetime\n",
    "x = datetime.datetime.now()\n",
    "date_str = x.strftime(\"%d\") + x.strftime(\"%b\") + x.strftime(\"%Y\")\n",
    "\n",
    "# Writting in an existing excel file\n",
    "from openpyxl import load_workbook\n",
    "book = load_workbook(filePath)\n",
    "writer = pd.ExcelWriter(filePath, engine = 'openpyxl')\n",
    "writer.book = book\n",
    "\n",
    "df.to_excel(writer, sheet_name='Updated_table_'+date_str) #sorted_df\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b23f79",
   "metadata": {},
   "source": [
    "# B2) Updating all postsynaptic neurons databases (loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc2480",
   "metadata": {},
   "source": [
    "## 1. Loading data bases of interest in a loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab634b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "# Choose path and file\n",
    "dataPath = r'Z:\\Further projects\\Heterogeneity across cell types\\data\\Excels\\drive-data-sets\\database' # write your path\n",
    "\n",
    "fileName_ls = glob(dataPath +\"\\\\\"+ \"*.xlsx\")\n",
    "\n",
    "\n",
    "#Creating the database in a loop\n",
    "df_ls = []\n",
    "for fileName in fileName_ls:\n",
    "    print(f'Importing: {fileName}')\n",
    "    filePath = os.path.join(dataPath,fileName)\n",
    "    df = pd.read_excel(filePath)\n",
    "    #Dropping the fisrt row ('asdf' was added as a walk-around to set that column values as type str)\n",
    "    if df[\"seg_id\"][0] == 'asdf': \n",
    "        df = df.iloc[1: , :]\n",
    "        df.reset_index(inplace=True,drop=True)\n",
    "    df_ls.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c266c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2d7f6e1",
   "metadata": {},
   "source": [
    "# C) Updating of list of IDs from a txt file / or a plain excell file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d87d5",
   "metadata": {},
   "source": [
    "## 1. Loading the data from a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc343d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose path and file\n",
    "dataPath = r'Z:\\Further projects\\Heterogeneity across cell types\\data\\Txts\\cell_type_poofread'# write your path\n",
    "fileDate = '20231106'\n",
    "fileName = f'root_ids_T4d_R_{fileDate}.txt'\n",
    "filePath = os.path.join(dataPath,fileName)\n",
    "ids_df = pd.read_csv(filePath, sep = \",\")\n",
    "curr_ID_ls = ids_df.columns.tolist()\n",
    "curr_ID_ls = [s for s in curr_ID_ls if \".1\" not in s]\n",
    "print(curr_ID_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483942e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0942b39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating the IDs via Fafbseg\n",
    "updated_ID_df = fafbseg.flywire.update_ids(curr_ID_ls, stop_layer=2, supervoxels=None, timestamp=None, dataset='production', progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f4b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_ID_df[updated_ID_df['confidence'] < 0.8]#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e048d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(updated_ID_df['new_id'].tolist())\n",
    "len(updated_ID_df['new_id'].unique().tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aabf6eb",
   "metadata": {},
   "source": [
    "## 2. Saving data in txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5ca97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the data\n",
    "\n",
    "updated_fileName = f'Updated_{fileName}'\n",
    "updated_filePath = os.path.join(dataPath,updated_fileName)\n",
    "\n",
    "id_list = list(set(updated_ID_df['new_id'].unique().tolist()))\n",
    "with open(updated_filePath , \"w\") as output:\n",
    "    output.write(str(id_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1c24d2",
   "metadata": {},
   "source": [
    "## 1. Or, loading the data from an excell file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425def5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose path and file\n",
    "\n",
    "dataPath = r'Z:\\Further projects\\Heterogeneity across cell types\\data\\Excels\\drive-data-sets\\database' # write your path\n",
    "fileName = f'Mi1 proofreadings.xlsx'\n",
    "filePath = os.path.join(dataPath,fileName)\n",
    "\n",
    "#Loading file as DataFrame\n",
    "df = pd.read_excel(filePath)\n",
    "\n",
    "\n",
    "#Dropping the fisrt row ('asdf' was added as a walk-around to set that column values as type str)\n",
    "if df[\"seg_id\"][0] == 'asdf': \n",
    "    df = df.iloc[1: , :]\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "#Dropping dupllicates\n",
    "result_df = df.drop_duplicates(subset=[\"Updated_seg_id\"], keep='first').copy()\n",
    "\n",
    "#Quick look on the dataframe\n",
    "display(result_df.head())\n",
    "    \n",
    "#Getting the lists of IDs to update\n",
    "curr_ID_ls = result_df[\"Updated_seg_id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4d0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating all IDs at once\n",
    "updated_ID_df = fafbseg.flywire.update_ids(curr_ID_ls, stop_layer=2, supervoxels=None, timestamp=None, dataset='production', progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536c231b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Or, Updating the IDs via Fafbseg in a for loop\n",
    "\n",
    "_start = 0\n",
    "_steps = 100\n",
    "_last = _steps\n",
    "_rounds =  round(len(curr_ID_ls)/ _steps) +1\n",
    "\n",
    "print(f'Rounds to perform: {_rounds}')\n",
    "updated_ID_df = pd.DataFrame()\n",
    "for i in range(0,_rounds):\n",
    "    #print(f'Round {i}')\n",
    "    curr_ID_df = fafbseg.flywire.update_ids(curr_ID_ls[_start:_last], stop_layer=2, supervoxels=None, timestamp=None, dataset='production', progress=True)\n",
    "    updated_ID_df = pd.concat([updated_ID_df,curr_ID_df])\n",
    "    _start = _start + _steps\n",
    "    _last = _last + _steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f7b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.reset_index(drop=True, inplace=True)\n",
    "result_df['Updated_seg_ids'] = updated_ID_df['new_id'].astype(str).tolist()\n",
    "result_df['Updated_confidence'] = updated_ID_df['confidence'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df81568e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(updated_ID_df.head())\n",
    "print('Update confidences: ')\n",
    "print(set(updated_ID_df['confidence'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5503cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_ID_df[updated_ID_df['confidence'] < 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb7a7b1",
   "metadata": {},
   "source": [
    "## 2. Saving back in the excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2903e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating string for the date\n",
    "import datetime\n",
    "x = datetime.datetime.now()\n",
    "date_str = x.strftime(\"%d\") + x.strftime(\"%b\") + x.strftime(\"%Y\")\n",
    "\n",
    "# Writting in an existing excel file\n",
    "from openpyxl import load_workbook\n",
    "book = load_workbook(filePath)\n",
    "writer = pd.ExcelWriter(filePath, engine = 'openpyxl')\n",
    "writer.book = book\n",
    "\n",
    "result_df.to_excel(writer, sheet_name='Updated_table_'+date_str) #sorted_df\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54539894",
   "metadata": {},
   "source": [
    "# E) Making a list of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f288081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_disc = 'D'\n",
    "dataPath =  f'{PC_disc}:\\Connectomics-Data\\FlyWire\\Txts'\n",
    "\n",
    "#File one\n",
    "fileName_1 = f'temp.txt'\n",
    "filePath = os.path.join(dataPath,fileName_1)\n",
    "df_1 = pd.read_csv(filePath, delimiter = \"\\t\",header=None)\n",
    "list1 = list(set(df_1[0].tolist()))\n",
    "\n",
    "# Saving function\n",
    "def save_list_to_file(file_path, input_list):\n",
    "    df = pd.DataFrame(input_list, columns=['Items'])\n",
    "    df.to_csv(file_path, header=False, index=False)\n",
    "    \n",
    "    \n",
    "PC_disc = 'D'\n",
    "\n",
    "file_path_1 = os.path.join(dataPath,f'Unique_{fileName_1}')\n",
    "save_list_to_file(file_path_1, list1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
