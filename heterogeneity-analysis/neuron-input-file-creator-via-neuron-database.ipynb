{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c84dbdb7",
   "metadata": {},
   "source": [
    "# Neuron input count file creator based on curated neuron database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9ba3a9",
   "metadata": {},
   "source": [
    "This notebook loads excel files and combines them into a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c5bcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import fafbseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some custom functions\n",
    "\n",
    "def update_dataframe_single_column(source_df, target_df, reference_column):\n",
    "    # Create a dictionary mapping from the reference column to the source DataFrame\n",
    "    reference_dict = source_df.groupby(reference_column).first().reset_index().to_dict(orient='records')\n",
    "    reference_dict = {row[reference_column]: row for row in reference_dict}\n",
    "\n",
    "    # Update the target DataFrame based on the reference column\n",
    "    for i, row in target_df.iterrows():\n",
    "        ref = row[reference_column]\n",
    "        if ref in reference_dict:\n",
    "            source_row = reference_dict[ref]\n",
    "            target_df.loc[i] = source_row\n",
    "\n",
    "    return target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b884f78a",
   "metadata": {},
   "source": [
    "### 1. Loading all data sets of interest in a loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7513a7",
   "metadata": {},
   "source": [
    "The original excell files need tzo be stored in the same folder. All files in that folder will be loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f521653a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data paths\n",
    "# Choose path and file\n",
    "dataPath = r'Z:\\Further projects\\Heterogeneity across cell types\\data\\Excels\\drive-data-sets\\database'\n",
    "\n",
    "fileName_ls = glob(dataPath +\"\\\\\"+ \"*.xlsx\")\n",
    "\n",
    "\n",
    "#Creating the database in a loop\n",
    "df_ls = []\n",
    "for fileName in fileName_ls:\n",
    "    print(f'Importing: {fileName}')\n",
    "    filePath = os.path.join(dataPath,fileName)\n",
    "    df = pd.read_excel(filePath)\n",
    "    #Dropping the fisrt row ('asdf' was added as a walk-around to set that column values as type str)\n",
    "    if df[\"seg_id\"][0] == 'asdf': \n",
    "        df = df.iloc[1: , :]\n",
    "        df.reset_index(inplace=True,drop=True)\n",
    "    df_ls.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fea6ce6",
   "metadata": {},
   "source": [
    "### 2. Creating, updating and filtering the database (db) of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9580be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Creation\n",
    "db = pd.concat(df_ls)\n",
    "print(f'\\n\\nCell types in the database: {db.symbol.unique()}, total = {len(db.symbol.unique())}')\n",
    "\n",
    "## Removing any Nan columns\n",
    "db = db[db[\"Updated_seg_id\"].notna()]\n",
    "\n",
    "### Filtering and updating database\n",
    "## Chossing optic lobe of interest:\n",
    "_hemisphere = 'R'\n",
    "db_R = db[db.hemisphere != 'L'].copy()\n",
    "\n",
    "## Updating segmnet ids\n",
    "#Getting the lists of IDs to update\n",
    "curr_ID_ls = db_R[\"Updated_seg_id\"].tolist()\n",
    "curr_ID_ls = db_R[\"seg_id\"].tolist()\n",
    "#Updating all IDs at once\n",
    "updated_ID_df = fafbseg.flywire.update_ids(curr_ID_ls, stop_layer=2, supervoxels=None, timestamp=None, \n",
    "                                           dataset='production', progress=True)\n",
    "db_R['Updated_seg_ids'] = updated_ID_df['new_id'].astype(str).tolist()\n",
    "db_R['Updated_confidence'] = updated_ID_df['confidence'].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19da5923",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_R['symbol'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a67d06",
   "metadata": {},
   "source": [
    "### 3. Creating a input file of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b891e6c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Selecting postsynaptic neuron of interest\n",
    "neuron = 'C3'\n",
    "neuron_df = db_R[db_R['symbol'] == neuron].copy()\n",
    "\n",
    "#Filtering for valid segment ids based on a given criteria\n",
    "neuron_selected_df = neuron_df.copy()\n",
    "neuron_selected_df = neuron_selected_df[neuron_selected_df['backbone proofread (Y/N)'] == 'Y'].copy()\n",
    "\n",
    "print(f'\\n\\nTotal number of postsynaptic cells: {len(neuron_selected_df)}\\n\\n')\n",
    "\n",
    "ID_ls = neuron_selected_df['Updated_seg_ids'].tolist()\n",
    "\n",
    "#Fetching the neuron's inputs and putputs\n",
    "neurons_inputs = fafbseg.flywire.synapses.fetch_synapses(ID_ls, pre=False, post=True, attach=True, \n",
    "                                             min_score=50, clean=True, transmitters=False, \n",
    "                                             neuropils=True, batch_size=30, \n",
    "                                             dataset='production', progress=True,mat= \"live\")\n",
    "\n",
    "neurons_outputs = fafbseg.flywire.synapses.fetch_synapses(ID_ls, pre=True, post=False, attach=True, \n",
    "                                             min_score=50, clean=True, transmitters=False, \n",
    "                                             neuropils=True, batch_size=30, \n",
    "                                             dataset='production', progress=True,mat= \"live\")\n",
    "\n",
    "\n",
    "#Counting inputs per ID, option joining dataframes\n",
    "final_input_df = pd.DataFrame()\n",
    "for n in neurons_inputs['post'].unique():\n",
    "    inputs_count = {}\n",
    "    curr_inputs = neurons_inputs[neurons_inputs['post'] == n]\n",
    "    inputs_str = curr_inputs.applymap(str)\n",
    "    \n",
    "    for c in inputs_str['pre'].to_list():\n",
    "        inputs_count[c] = inputs_count.get(c, 0) + 1\n",
    "    input_count_df = pd.DataFrame(inputs_count, index=[0])\n",
    "    input_count_df = input_count_df.T\n",
    "    input_count_df.rename(columns={0: \"counts\"},inplace=True)\n",
    "    input_count_df.index.names = ['presynaptic_ID']\n",
    "    input_count_df = input_count_df.sort_values(by=\"counts\",ascending=False)\n",
    "    input_count_df['postsynaptic_ID'] = inputs_str['post'].to_list()[0:len(input_count_df)]\n",
    "    final_input_df = final_input_df.append(input_count_df)\n",
    "    #print(f'Counting done for: {n}')\n",
    "input_count_str_df = final_input_df.applymap(str)\n",
    "print('INPUTS: ')\n",
    "display(input_count_str_df.head())\n",
    "\n",
    "\n",
    "#Counting outputs per ID, option joining dataframes\n",
    "final_output_df = pd.DataFrame()\n",
    "for n in neurons_outputs['pre'].unique():\n",
    "    outputs_count = {}\n",
    "    curr_outputs = neurons_outputs[neurons_outputs['pre'] == n]\n",
    "    outputs_str = curr_outputs.applymap(str)\n",
    "    \n",
    "    for c in outputs_str['post'].to_list():\n",
    "        outputs_count[c] = outputs_count.get(c, 0) + 1\n",
    "    output_count_df = pd.DataFrame(outputs_count, index=[0])\n",
    "    output_count_df = output_count_df.T\n",
    "    output_count_df.rename(columns={0: \"counts\"},inplace=True)\n",
    "    output_count_df.index.names = ['postsynaptic_ID']\n",
    "    output_count_df = output_count_df.sort_values(by=\"counts\",ascending=False)\n",
    "    output_count_df['presynaptic_ID'] = outputs_str['pre'].to_list()[0:len(output_count_df)]\n",
    "    final_output_df = final_output_df.append(output_count_df)\n",
    "    #print(f'Counting done for: {n}')\n",
    "output_count_str_df = final_output_df.applymap(str)\n",
    "print('OUTPUTS: ')\n",
    "display(output_count_str_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00dfafc",
   "metadata": {},
   "source": [
    "### 4. Adding useful information to the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a344b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For INPUTS\n",
    "\n",
    "\n",
    "# Selecting dataframe\n",
    "#Updating the IDs via Fafbseg\n",
    "partner_ID = input_count_str_df.index.tolist()\n",
    "updated_ID_df = fafbseg.flywire.update_ids(partner_ID, stop_layer=2, supervoxels=None, timestamp=None, dataset='production', progress=True)\n",
    "partner_ID_ls = updated_ID_df[\"new_id\"].tolist()\n",
    "\n",
    "# Identifying user-based annotations about cell identity\n",
    "\n",
    "identification_df = fafbseg.flywire.find_celltypes(partner_ID_ls, user=None, exact=False, case=False, regex=True, update_roots=False)\n",
    "identification_no_duplicates_df = identification_df.drop_duplicates(subset='pt_root_id', keep='last', inplace=False, ignore_index=False).copy()\n",
    "\n",
    "# Adding info to the current data set\n",
    "\n",
    "# Selecting dataframes and resetting index\n",
    "source_df = identification_no_duplicates_df.copy()\n",
    "source_df.reset_index(inplace = True, drop = True)\n",
    "target_df = input_count_str_df.copy()\n",
    "target_df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "\n",
    "# Adding columns for the function to properly work\n",
    "target_df['presynaptic_ID'] = input_count_str_df.index.astype(str)\n",
    "source_df['presynaptic_ID'] = identification_no_duplicates_df['pt_root_id'].tolist()\n",
    "target_df['guess'] = None\n",
    "source_df['guess'] = identification_no_duplicates_df['tag'].tolist()\n",
    "target_df['author'] = None\n",
    "source_df['author'] = identification_no_duplicates_df['user_id'].tolist()\n",
    "\n",
    "# Function inputs\n",
    "source_cols = ['guess', 'author','presynaptic_ID']\n",
    "target_cols = ['guess', 'author', 'presynaptic_ID']\n",
    "reference_column = 'presynaptic_ID'\n",
    "\n",
    "source_df = source_df[source_cols].copy()\n",
    "target_df = target_df[source_cols].copy()\n",
    "\n",
    "source_df = source_df.astype(str)\n",
    "target_df = target_df.astype(str)\n",
    "\n",
    "\n",
    "# Running the function and compleating the dataset\n",
    "result_input_df = update_dataframe_single_column(source_df, target_df,reference_column)\n",
    "result_input_df['counts'] = input_count_str_df['counts'].tolist()\n",
    "result_input_df['postsynaptic_ID'] = input_count_str_df['postsynaptic_ID'].tolist()\n",
    "result_input_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9885c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For OUTPUTS\n",
    "\n",
    "# Selecting dataframe\n",
    "#Updating the IDs via Fafbseg\n",
    "partner_ID = output_count_str_df.index.tolist()\n",
    "updated_ID_df = fafbseg.flywire.update_ids(partner_ID, stop_layer=2, supervoxels=None, timestamp=None, dataset='production', progress=True)\n",
    "partner_ID_ls = updated_ID_df[\"new_id\"].tolist()\n",
    "\n",
    "# Identifying user-based annotations about cell identity\n",
    "\n",
    "identification_df = fafbseg.flywire.find_celltypes(partner_ID_ls, user=None, exact=False, case=False, regex=True, update_roots=False)\n",
    "identification_no_duplicates_df = identification_df.drop_duplicates(subset='pt_root_id', keep='last', inplace=False, ignore_index=False).copy()\n",
    "\n",
    "# Adding info to the current data set\n",
    "\n",
    "# Selecting dataframes and resetting index\n",
    "source_df = identification_no_duplicates_df.copy()\n",
    "source_df.reset_index(inplace = True, drop = True)\n",
    "target_df = output_count_str_df.copy()\n",
    "target_df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "\n",
    "# Adding columns for the function to properly work\n",
    "target_df['postsynaptic_ID'] = output_count_str_df.index.astype(str)\n",
    "source_df['postsynaptic_ID'] = identification_no_duplicates_df['pt_root_id'].tolist()\n",
    "target_df['guess'] = None\n",
    "source_df['guess'] = identification_no_duplicates_df['tag'].tolist()\n",
    "target_df['author'] = None\n",
    "source_df['author'] = identification_no_duplicates_df['user_id'].tolist()\n",
    "\n",
    "# Function inputs\n",
    "source_cols = ['guess', 'author','postsynaptic_ID']\n",
    "target_cols = ['guess', 'author', 'postsynaptic_ID']\n",
    "reference_column = 'postsynaptic_ID'\n",
    "\n",
    "source_df = source_df[source_cols].copy()\n",
    "target_df = target_df[source_cols].copy()\n",
    "\n",
    "source_df = source_df.astype(str)\n",
    "target_df = target_df.astype(str)\n",
    "\n",
    "\n",
    "# Running the function and compleating the dataset\n",
    "result_output_df = update_dataframe_single_column(source_df, target_df,reference_column)\n",
    "result_output_df['counts'] = output_count_str_df['counts'].tolist()\n",
    "result_output_df['presynaptic_ID'] = output_count_str_df['presynaptic_ID'].tolist()\n",
    "result_output_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f9084",
   "metadata": {},
   "source": [
    "### 5. Transfering information from the main database (db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96bf262",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For INPUTS\n",
    "\n",
    "# Matcing data types\n",
    "db_R['Updated_seg_id'] = db_R['Updated_seg_id'].astype(str)\n",
    "result_input_df['presynaptic_ID'] = result_input_df['presynaptic_ID'].astype(str)\n",
    "\n",
    "# Merging the DataFrames based on common values\n",
    "merged_input_df = pd.merge(result_input_df, db_R[['Updated_seg_id', 'symbol']], left_on='presynaptic_ID', right_on='Updated_seg_id', how='left')\n",
    "\n",
    "# Drop the extra 'seg_id' column\n",
    "merged_input_df.drop(columns=['Updated_seg_id'], inplace=True)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print('For INPUTS')\n",
    "display(merged_input_df)\n",
    "\n",
    "\n",
    "## For OUTPUTS\n",
    "\n",
    "# Matcing data types\n",
    "db_R['Updated_seg_id'] = db_R['Updated_seg_id'].astype(str)\n",
    "result_output_df['postynaptic_ID'] = result_output_df['postsynaptic_ID'].astype(str)\n",
    "\n",
    "# Merging the DataFrames based on common values\n",
    "merged_output_df = pd.merge(result_output_df, db_R[['Updated_seg_id', 'symbol']], left_on='postsynaptic_ID', right_on='Updated_seg_id', how='left')\n",
    "\n",
    "# Drop the extra 'seg_id' column\n",
    "merged_output_df.drop(columns=['Updated_seg_id'], inplace=True)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print('For OUTPUTS')\n",
    "display(merged_output_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d2e7c1",
   "metadata": {},
   "source": [
    "### 6. Adding more useful information for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For INPUTS\n",
    "\n",
    "# Matcing data types\n",
    "neuron_df['Updated_seg_id'] = neuron_df['Updated_seg_id'].astype(str)\n",
    "merged_input_df['postsynaptic_ID'] = merged_input_df['postsynaptic_ID'].astype(str)\n",
    "\n",
    "# Merging the DataFrames based on common values\n",
    "merged_input_2_df = pd.merge(merged_input_df, neuron_df[['Updated_seg_id', 'optic_lobe_id','dorso-ventral']], left_on='postsynaptic_ID', right_on='Updated_seg_id', how='left')\n",
    "\n",
    "# Drop the extra 'seg_id' column\n",
    "merged_input_2_df.drop(columns=['Updated_seg_id'], inplace=True)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print('For INPUTS:')\n",
    "display(merged_input_2_df)\n",
    "\n",
    "\n",
    "## For OUTPUTS\n",
    "\n",
    "# Matcing data types\n",
    "neuron_df['Updated_seg_id'] = neuron_df['Updated_seg_id'].astype(str)\n",
    "merged_output_df['presynaptic_ID'] = merged_output_df['presynaptic_ID'].astype(str)\n",
    "\n",
    "# Merging the DataFrames based on common values\n",
    "merged_output_2_df = pd.merge(merged_output_df, neuron_df[['Updated_seg_id', 'optic_lobe_id','dorso-ventral']], left_on='presynaptic_ID', right_on='Updated_seg_id', how='left')\n",
    "\n",
    "# Drop the extra 'seg_id' column\n",
    "merged_output_2_df.drop(columns=['Updated_seg_id'], inplace=True)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print('For OUTPUTS:')\n",
    "display(merged_output_2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1705051b",
   "metadata": {},
   "source": [
    "### Saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79fa75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data in your computer\n",
    "outDir = r'Z:\\Further projects\\Heterogeneity across cell types\\data\\Excels\\min-score-50' # YOUR-PATH for saving excel file\n",
    "save_excel_file = True\n",
    "\n",
    "import datetime\n",
    "x = datetime.datetime.now()\n",
    "date_str = x.strftime(\"%d\") + x.strftime(\"%b\") + x.strftime(\"%Y\")\n",
    "\n",
    "if save_excel_file: \n",
    "    ## Input count\n",
    "    file_name = f'{neuron}_neurons_input_count_{_hemisphere}_{date_str}.xlsx'\n",
    "    savePath = os.path.join(outDir, file_name)\n",
    "    merged_input_2_df.to_excel(savePath, sheet_name='Buhmann synapses, inputs')\n",
    "    \n",
    "    ## Output count\n",
    "    file_name = f'{neuron}_neurons_output_count_{_hemisphere}_{date_str}.xlsx'\n",
    "    savePath = os.path.join(outDir, file_name)\n",
    "    merged_output_2_df.to_excel(savePath, sheet_name='Buhmann synapses, outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c38f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
